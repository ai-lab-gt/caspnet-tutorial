{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ./model/capsnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "# from tensorboardX import SummaryWriter\n",
    "from tqdm import *\n",
    "\n",
    "def squash(input):\n",
    "    \"\"\"\n",
    "    Squashing function for a tensor.\n",
    "    :param input: torch.Tensor\n",
    "    \"\"\"\n",
    "    assert (input.norm() > 0), \"Division by zero in second term of equation\"\n",
    "    norm = input.norm()\n",
    "    squared_norm = norm.pow(2)\n",
    "    return (squared_norm/(1+squared_norm))*(input/norm)\n",
    "\n",
    "\n",
    "class PrimaryCapsules(nn.Module):\n",
    "    \"\"\"\n",
    "    Primary Capsule Network on MNIST.\n",
    "    :param conv1_params: Parameters for first Conv2d layer\n",
    "    :param conv2_params: Parameters for second Conv2d layer\n",
    "    :param caps_maps: number of feature maps (capsules)\n",
    "    :param caps_dims: dimension of each capsule's activation vector\n",
    "    Shape:\n",
    "        - Input: (batch, channels, height, width)\n",
    "        - Output: (batch, n_caps, caps_dims)\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_params, caps_maps=32, num_capsules=8):\n",
    "        super(PrimaryCapsules, self).__init__()\n",
    "        # Output of conv2 has 256 (32*8) maps of 6x6.\n",
    "        # We instead want 32 vectors of 8 dims each.\n",
    "        self.num_routes = caps_maps * 6 * 6\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv2d(**conv_params) for _ in range(self.num_capsules)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = [capsule(x) for capsule in self.capsules]\n",
    "        output = torch.stack(output, dim=1)\n",
    "        output = output.view(x.size(0), self.num_routes, -1)\n",
    "        return squash(output)\n",
    "\n",
    "# https://github.com/laubonghaudoi/CapsNet_guide_PyTorch/blob/master/DigitCaps.py\n",
    "class DigitCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    Digit Capsule Layer.\n",
    "    :param num_lower_capsules: Number of lower level capsules, used to calculate dynamic routing.\n",
    "    Shape:\n",
    "        - Input: (batch, channels, height, width)\n",
    "        - Output: (batch, n_caps, caps_dims)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_route_nodes, in_channels, out_channels, num_iterations):\n",
    "        super(DigitCapsule, self).__init__()\n",
    "        self.num_capsules = 10\n",
    "        self.num_route_nodes = num_route_nodes\n",
    "        self.num_iterations = num_iterations\n",
    "        # W.shape => [1, 10, 1152, 8, 16]\n",
    "        # (1) is to be broadcastable with torch.matmul\n",
    "        self.W = nn.Parameter(torch.randn(1,\n",
    "                                        self.num_route_nodes,\n",
    "                                        self.num_capsules,\n",
    "                                        out_channels,\n",
    "                                        in_channels,\n",
    "                                        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        u_hat = torch.matmul(W, x)\n",
    "\n",
    "        # Routing Algorithm.\n",
    "        # for all capsule i in layer l and capsule j in layer (l + 1): b_ij ← 0\n",
    "        b = Variable(torch.zeros(1, self.num_route_nodes, self.num_capsules, 1))\n",
    "        # for r iterations do\n",
    "        for i in range(self.num_iterations):\n",
    "            # for all capsule i in layer l: c_i ← softmax(b_i)\n",
    "            c = F.softmax(b)\n",
    "            c = torch.cat([c] * batch_size, dim=0).unsqueeze(4)\n",
    "            # for all capsule j in layer (l+1): s_j ← SUM_i ( c_ij * u_hat_j|i )\n",
    "            s = (u_hat * c).sum(dim=1, keepdim=True)\n",
    "            # for all capsule j in layer (l + 1): v_j ← squash(s_j)\n",
    "            v = squash(s)\n",
    "            # for all capsule i in layer l and capsule j in layer (l + 1): b_ij ← b_ij + u_hat_j|i * v_j\n",
    "            if i < self.num_iterations - 1:\n",
    "                a = torch.matmul(u_hat.transpose(3, 4), torch.cat([v] * self.num_route_nodes, dim=1))\n",
    "                b = b + a.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v.squeeze(1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.reconstraction_layers = nn.Sequential(\n",
    "            nn.Linear(16 * 10, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, data):\n",
    "        classes = torch.sqrt((x ** 2).sum(2))\n",
    "        classes = F.softmax(classes)\n",
    "        \n",
    "        _, max_length_indices = classes.max(dim=1)\n",
    "        masked = Variable(torch.sparse.torch.eye(10))\n",
    "        masked = masked.index_select(dim=0, index=max_length_indices.squeeze(1).data)\n",
    "        \n",
    "        reconstructions = self.reconstraction_layers((x * masked[:, :, None, None]).view(x.size(0), -1))\n",
    "        reconstructions = reconstructions.view(-1, 1, 28, 28)\n",
    "        \n",
    "        return reconstructions, masked\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, conv1_params, conv2_params):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(**conv1_params)\n",
    "        self.primary_capsules = PrimaryCapsules(conv2_params, num_capsules=8)\n",
    "        self.digit_capsules = DigitCapsule(num_route_nodes=32*6*6, in_channels=8, out_channels=16, num_iterations=3)\n",
    "        self.decoder = Decoder()\n",
    "        # self.decoder = nn.Sequential(\n",
    "        #     nn.Linear(16 * 10, 512),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Linear(512, 1024),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Linear(1024, 784),\n",
    "        #     nn.Sigmoid()\n",
    "        # ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        u = self.primary_capsules(x)\n",
    "        v = self.digit_capsules(u)\n",
    "        print('V size', v.size())\n",
    "        reconstruction, masked = self.decoder(v, x)\n",
    "        print('reconstruction size', reconstruction.size())\n",
    "        return v, reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ./model/loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def reconstruction_loss(v, target, image):\n",
    "    pass\n",
    "\n",
    "def margin_loss(v, target, batch_size):\n",
    "    l = 0.5\n",
    "    m = 0.9\n",
    "    T = target.type(torch.FloatTensor)\n",
    "    norm = torch.norm(v)\n",
    "    zeros = Variable(torch.zeros(norm.size()))\n",
    "    # L_k = T_k max(0, m^+ − ||v_k||)^2 + λ (1 − T_k) max(0, ||v_k|| − m^−)^2\n",
    "    L = T * torch.max(zeros, m - norm) ** 2 + l * (1 -T) * torch.max(zeros, norm - (1. - m)) ** 2\n",
    "    return torch.sum(L) / batch_size\n",
    "\n",
    "\n",
    "def loss(v, target, batch_size):\n",
    "    return margin_loss(v, target, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ./train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import tqdm\n",
    "\n",
    "from model.capsnet import CapsNet\n",
    "from model.loss import loss\n",
    "\n",
    "def train(model, epochs=100, dataset='mnist', lr=0.001):\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    data_dir = \"./data/\"\n",
    "\n",
    "    if dataset == 'mnist':\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST(data_dir, train=True, download=True,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.30801,))\n",
    "                        ])),\n",
    "            batch_size=64, shuffle=True)\n",
    "    elif dataset == 'fashion-mnist':\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            datasets.FashionMNIST(data_dir, train=True, download=True,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.30801,))\n",
    "                        ])),\n",
    "            batch_size=64, shuffle=True)\n",
    "    else:\n",
    "        print('Only accepts mnist | fashion-mnist')\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            test_sample = data\n",
    "            batch_size = test_sample.size()[0]\n",
    "            # print(f\"Sample size: {test_sample.size()}\")\n",
    "            output, reconstruction = model(data)\n",
    "            L = loss(output, target, batch_size)\n",
    "            L.backward()\n",
    "\n",
    "            step = batch_idx + epoch\n",
    "            if epoch % 10 == 0:\n",
    "                tqdm.tqdm.write(f'Epoch: {step}    Loss: {L.data.item()}')\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "conv1_params = {\n",
    "    \"in_channels\": 1,\n",
    "    \"out_channels\": 256,\n",
    "    \"kernel_size\": 9,\n",
    "    \"stride\": 1\n",
    "}\n",
    "\n",
    "conv2_params = {\n",
    "    \"in_channels\": 256,\n",
    "    \"out_channels\": 32,\n",
    "    \"kernel_size\": 9,\n",
    "    \"stride\": 2,\n",
    "    \"padding\": 0,\n",
    "}\n",
    "\n",
    "# NOTE. What parameters would we like to experiment with?\n",
    "# num of capsules in PrimaryCaps? Capsule Dimensions? Conv params?\n",
    "model = CapsNet(conv1_params, conv2_params)\n",
    "train(model)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
